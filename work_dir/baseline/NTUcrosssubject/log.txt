[ Fri Apr 20 19:50:47 2018 ] Parameters:
{'work_dir': 'work_dir/baseline/NTUcrosssubject', 'config': 'config/baseline/nturgbd-cross-subject/train.yaml', 'phase': 'train', 'save_score': False, 'seed': 1, 'log_interval': 100, 'save_interval': 10, 'eval_interval': 5, 'print_log': True, 'show_topk': [1, 5], 'feeder': 'st_gcn.feeder.Feeder', 'num_worker': 128, 'train_feeder_args': {'mode': 'train', 'window_size': 300, 'data_path': './data/NTU-RGB-D/xsub/train_data.npy', 'label_path': './data/NTU-RGB-D/xsub/train_label.pkl'}, 'test_feeder_args': {'mode': 'test', 'window_size': 300, 'data_path': './data/NTU-RGB-D/xsub/val_data.npy', 'label_path': './data/NTU-RGB-D/xsub/val_label.pkl'}, 'model': 'st_gcn.net.TCN', 'model_args': {'num_class': 60, 'channel': 150, 'window_size': 300, 'use_data_bn': True}, 'weights': None, 'ignore_weights': [], 'base_lr': 0.1, 'step': [10, 70], 'device': [0], 'optimizer': 'SGD', 'nesterov': True, 'batch_size': 64, 'test_batch_size': 64, 'start_epoch': 0, 'num_epoch': 80, 'weight_decay': 0.0001}

[ Fri Apr 20 19:50:47 2018 ] Training epoch: 1
[ Fri Apr 20 19:51:08 2018 ] 	Batch(0/627) done. Loss: 4.0961  lr:0.100000
[ Fri Apr 20 19:51:19 2018 ] 	Batch(100/627) done. Loss: 3.7876  lr:0.100000
[ Fri Apr 20 19:51:29 2018 ] 	Batch(200/627) done. Loss: 3.4007  lr:0.100000
[ Fri Apr 20 19:51:39 2018 ] 	Batch(300/627) done. Loss: 2.8861  lr:0.100000
[ Fri Apr 20 19:51:50 2018 ] 	Batch(400/627) done. Loss: 2.6133  lr:0.100000
[ Fri Apr 20 19:52:00 2018 ] 	Batch(500/627) done. Loss: 2.5300  lr:0.100000
[ Fri Apr 20 19:52:10 2018 ] 	Batch(600/627) done. Loss: 2.3989  lr:0.100000
[ Fri Apr 20 19:52:13 2018 ] 	Mean training loss: 3.0885.
[ Fri Apr 20 19:52:13 2018 ] 	Time consumption: [Data]21%, [Network]79%
[ Fri Apr 20 19:52:13 2018 ] Training epoch: 2
[ Fri Apr 20 19:52:30 2018 ] 	Batch(0/627) done. Loss: 2.4345  lr:0.100000
[ Fri Apr 20 19:52:40 2018 ] 	Batch(100/627) done. Loss: 2.2114  lr:0.100000
[ Fri Apr 20 19:52:49 2018 ] 	Batch(200/627) done. Loss: 2.0581  lr:0.100000
[ Fri Apr 20 19:52:59 2018 ] 	Batch(300/627) done. Loss: 2.3887  lr:0.100000
[ Fri Apr 20 19:53:09 2018 ] 	Batch(400/627) done. Loss: 2.0604  lr:0.100000
[ Fri Apr 20 19:53:15 2018 ] 	Batch(500/627) done. Loss: 1.9140  lr:0.100000
[ Fri Apr 20 19:53:23 2018 ] 	Batch(600/627) done. Loss: 1.7388  lr:0.100000
[ Fri Apr 20 19:53:25 2018 ] 	Mean training loss: 2.1246.
[ Fri Apr 20 19:53:25 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 19:53:25 2018 ] Training epoch: 3
[ Fri Apr 20 19:53:43 2018 ] 	Batch(0/627) done. Loss: 1.7833  lr:0.100000
[ Fri Apr 20 19:53:51 2018 ] 	Batch(100/627) done. Loss: 1.4169  lr:0.100000
[ Fri Apr 20 19:54:01 2018 ] 	Batch(200/627) done. Loss: 1.7831  lr:0.100000
[ Fri Apr 20 19:54:11 2018 ] 	Batch(300/627) done. Loss: 1.5440  lr:0.100000
[ Fri Apr 20 19:54:22 2018 ] 	Batch(400/627) done. Loss: 1.7838  lr:0.100000
[ Fri Apr 20 19:54:32 2018 ] 	Batch(500/627) done. Loss: 1.5753  lr:0.100000
[ Fri Apr 20 19:54:42 2018 ] 	Batch(600/627) done. Loss: 1.8185  lr:0.100000
[ Fri Apr 20 19:54:45 2018 ] 	Mean training loss: 1.7253.
[ Fri Apr 20 19:54:45 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 19:54:45 2018 ] Training epoch: 4
[ Fri Apr 20 19:55:02 2018 ] 	Batch(0/627) done. Loss: 1.6628  lr:0.100000
[ Fri Apr 20 19:55:12 2018 ] 	Batch(100/627) done. Loss: 1.7018  lr:0.100000
[ Fri Apr 20 19:55:21 2018 ] 	Batch(200/627) done. Loss: 1.8608  lr:0.100000
[ Fri Apr 20 19:55:31 2018 ] 	Batch(300/627) done. Loss: 1.7603  lr:0.100000
[ Fri Apr 20 19:55:41 2018 ] 	Batch(400/627) done. Loss: 1.2151  lr:0.100000
[ Fri Apr 20 19:55:51 2018 ] 	Batch(500/627) done. Loss: 1.3684  lr:0.100000
[ Fri Apr 20 19:56:01 2018 ] 	Batch(600/627) done. Loss: 1.3598  lr:0.100000
[ Fri Apr 20 19:56:03 2018 ] 	Mean training loss: 1.5361.
[ Fri Apr 20 19:56:03 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 19:56:03 2018 ] Training epoch: 5
[ Fri Apr 20 19:56:20 2018 ] 	Batch(0/627) done. Loss: 1.3553  lr:0.100000
[ Fri Apr 20 19:56:30 2018 ] 	Batch(100/627) done. Loss: 1.5448  lr:0.100000
[ Fri Apr 20 19:56:41 2018 ] 	Batch(200/627) done. Loss: 1.6345  lr:0.100000
[ Fri Apr 20 19:56:50 2018 ] 	Batch(300/627) done. Loss: 1.4721  lr:0.100000
[ Fri Apr 20 19:56:58 2018 ] 	Batch(400/627) done. Loss: 1.2464  lr:0.100000
[ Fri Apr 20 19:57:07 2018 ] 	Batch(500/627) done. Loss: 1.0369  lr:0.100000
[ Fri Apr 20 19:57:16 2018 ] 	Batch(600/627) done. Loss: 1.6974  lr:0.100000
[ Fri Apr 20 19:57:18 2018 ] 	Mean training loss: 1.4084.
[ Fri Apr 20 19:57:18 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 19:57:18 2018 ] Eval epoch: 5
[ Fri Apr 20 19:57:39 2018 ] 	Mean test loss of 258 batches: 1.4245806361815727.
[ Fri Apr 20 19:57:41 2018 ] 	Top1: 58.05%
[ Fri Apr 20 19:57:41 2018 ] 	Top5: 88.34%
[ Fri Apr 20 19:57:41 2018 ] Training epoch: 6
[ Fri Apr 20 19:57:56 2018 ] 	Batch(0/627) done. Loss: 1.3186  lr:0.100000
[ Fri Apr 20 19:58:06 2018 ] 	Batch(100/627) done. Loss: 1.1815  lr:0.100000
[ Fri Apr 20 19:58:16 2018 ] 	Batch(200/627) done. Loss: 1.1163  lr:0.100000
[ Fri Apr 20 19:58:25 2018 ] 	Batch(300/627) done. Loss: 1.3658  lr:0.100000
[ Fri Apr 20 19:58:33 2018 ] 	Batch(400/627) done. Loss: 1.2798  lr:0.100000
[ Fri Apr 20 19:58:42 2018 ] 	Batch(500/627) done. Loss: 1.8970  lr:0.100000
[ Fri Apr 20 19:58:50 2018 ] 	Batch(600/627) done. Loss: 1.2615  lr:0.100000
[ Fri Apr 20 19:58:53 2018 ] 	Mean training loss: 1.3165.
[ Fri Apr 20 19:58:53 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 19:58:53 2018 ] Training epoch: 7
[ Fri Apr 20 19:59:10 2018 ] 	Batch(0/627) done. Loss: 1.4532  lr:0.100000
[ Fri Apr 20 19:59:21 2018 ] 	Batch(100/627) done. Loss: 1.1542  lr:0.100000
[ Fri Apr 20 19:59:31 2018 ] 	Batch(200/627) done. Loss: 1.2120  lr:0.100000
[ Fri Apr 20 19:59:41 2018 ] 	Batch(300/627) done. Loss: 0.9157  lr:0.100000
[ Fri Apr 20 19:59:51 2018 ] 	Batch(400/627) done. Loss: 1.0773  lr:0.100000
[ Fri Apr 20 20:00:02 2018 ] 	Batch(500/627) done. Loss: 1.5535  lr:0.100000
[ Fri Apr 20 20:00:12 2018 ] 	Batch(600/627) done. Loss: 0.8304  lr:0.100000
[ Fri Apr 20 20:00:14 2018 ] 	Mean training loss: 1.2515.
[ Fri Apr 20 20:00:14 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:00:14 2018 ] Training epoch: 8
[ Fri Apr 20 20:00:32 2018 ] 	Batch(0/627) done. Loss: 0.9993  lr:0.100000
[ Fri Apr 20 20:00:41 2018 ] 	Batch(100/627) done. Loss: 1.0331  lr:0.100000
[ Fri Apr 20 20:00:50 2018 ] 	Batch(200/627) done. Loss: 1.3496  lr:0.100000
[ Fri Apr 20 20:00:57 2018 ] 	Batch(300/627) done. Loss: 0.9429  lr:0.100000
[ Fri Apr 20 20:01:07 2018 ] 	Batch(400/627) done. Loss: 1.4520  lr:0.100000
[ Fri Apr 20 20:01:15 2018 ] 	Batch(500/627) done. Loss: 1.0196  lr:0.100000
[ Fri Apr 20 20:01:23 2018 ] 	Batch(600/627) done. Loss: 0.9698  lr:0.100000
[ Fri Apr 20 20:01:26 2018 ] 	Mean training loss: 1.1952.
[ Fri Apr 20 20:01:26 2018 ] 	Time consumption: [Data]28%, [Network]72%
[ Fri Apr 20 20:01:26 2018 ] Training epoch: 9
[ Fri Apr 20 20:01:44 2018 ] 	Batch(0/627) done. Loss: 1.4465  lr:0.100000
[ Fri Apr 20 20:01:54 2018 ] 	Batch(100/627) done. Loss: 1.4863  lr:0.100000
[ Fri Apr 20 20:02:04 2018 ] 	Batch(200/627) done. Loss: 1.4020  lr:0.100000
[ Fri Apr 20 20:02:14 2018 ] 	Batch(300/627) done. Loss: 1.3510  lr:0.100000
[ Fri Apr 20 20:02:24 2018 ] 	Batch(400/627) done. Loss: 1.3522  lr:0.100000
[ Fri Apr 20 20:02:34 2018 ] 	Batch(500/627) done. Loss: 0.9494  lr:0.100000
[ Fri Apr 20 20:02:45 2018 ] 	Batch(600/627) done. Loss: 1.1841  lr:0.100000
[ Fri Apr 20 20:02:47 2018 ] 	Mean training loss: 1.1457.
[ Fri Apr 20 20:02:49 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:02:49 2018 ] Training epoch: 10
[ Fri Apr 20 20:03:04 2018 ] 	Batch(0/627) done. Loss: 1.0539  lr:0.100000
[ Fri Apr 20 20:03:14 2018 ] 	Batch(100/627) done. Loss: 0.9604  lr:0.100000
[ Fri Apr 20 20:03:25 2018 ] 	Batch(200/627) done. Loss: 1.0176  lr:0.100000
[ Fri Apr 20 20:03:35 2018 ] 	Batch(300/627) done. Loss: 1.2289  lr:0.100000
[ Fri Apr 20 20:03:45 2018 ] 	Batch(400/627) done. Loss: 1.0073  lr:0.100000
[ Fri Apr 20 20:03:55 2018 ] 	Batch(500/627) done. Loss: 1.4924  lr:0.100000
[ Fri Apr 20 20:04:06 2018 ] 	Batch(600/627) done. Loss: 0.6963  lr:0.100000
[ Fri Apr 20 20:04:08 2018 ] 	Mean training loss: 1.1114.
[ Fri Apr 20 20:04:08 2018 ] 	Time consumption: [Data]23%, [Network]77%
[ Fri Apr 20 20:04:08 2018 ] Eval epoch: 10
[ Fri Apr 20 20:04:31 2018 ] 	Mean test loss of 258 batches: 1.2719739446806353.
[ Fri Apr 20 20:04:31 2018 ] 	Top1: 62.35%
[ Fri Apr 20 20:04:31 2018 ] 	Top5: 90.48%
[ Fri Apr 20 20:04:31 2018 ] Training epoch: 11
[ Fri Apr 20 20:04:49 2018 ] 	Batch(0/627) done. Loss: 1.2007  lr:0.010000
[ Fri Apr 20 20:04:59 2018 ] 	Batch(100/627) done. Loss: 0.7814  lr:0.010000
[ Fri Apr 20 20:05:09 2018 ] 	Batch(200/627) done. Loss: 0.4631  lr:0.010000
[ Fri Apr 20 20:05:20 2018 ] 	Batch(300/627) done. Loss: 0.8196  lr:0.010000
[ Fri Apr 20 20:05:30 2018 ] 	Batch(400/627) done. Loss: 0.8236  lr:0.010000
[ Fri Apr 20 20:05:39 2018 ] 	Batch(500/627) done. Loss: 0.9593  lr:0.010000
[ Fri Apr 20 20:05:49 2018 ] 	Batch(600/627) done. Loss: 1.0781  lr:0.010000
[ Fri Apr 20 20:05:51 2018 ] 	Mean training loss: 0.9072.
[ Fri Apr 20 20:05:51 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:05:51 2018 ] Training epoch: 12
[ Fri Apr 20 20:06:08 2018 ] 	Batch(0/627) done. Loss: 0.7718  lr:0.010000
[ Fri Apr 20 20:06:19 2018 ] 	Batch(100/627) done. Loss: 0.9309  lr:0.010000
[ Fri Apr 20 20:06:29 2018 ] 	Batch(200/627) done. Loss: 0.8318  lr:0.010000
[ Fri Apr 20 20:06:39 2018 ] 	Batch(300/627) done. Loss: 0.7941  lr:0.010000
[ Fri Apr 20 20:06:49 2018 ] 	Batch(400/627) done. Loss: 0.7447  lr:0.010000
[ Fri Apr 20 20:06:59 2018 ] 	Batch(500/627) done. Loss: 0.9650  lr:0.010000
[ Fri Apr 20 20:07:08 2018 ] 	Batch(600/627) done. Loss: 0.9040  lr:0.010000
[ Fri Apr 20 20:07:11 2018 ] 	Mean training loss: 0.8625.
[ Fri Apr 20 20:07:11 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:07:11 2018 ] Training epoch: 13
[ Fri Apr 20 20:07:28 2018 ] 	Batch(0/627) done. Loss: 0.9460  lr:0.010000
[ Fri Apr 20 20:07:38 2018 ] 	Batch(100/627) done. Loss: 0.9878  lr:0.010000
[ Fri Apr 20 20:07:48 2018 ] 	Batch(200/627) done. Loss: 0.7448  lr:0.010000
[ Fri Apr 20 20:07:58 2018 ] 	Batch(300/627) done. Loss: 0.8486  lr:0.010000
[ Fri Apr 20 20:08:07 2018 ] 	Batch(400/627) done. Loss: 0.5909  lr:0.010000
[ Fri Apr 20 20:08:18 2018 ] 	Batch(500/627) done. Loss: 1.0349  lr:0.010000
[ Fri Apr 20 20:08:27 2018 ] 	Batch(600/627) done. Loss: 0.8063  lr:0.010000
[ Fri Apr 20 20:08:30 2018 ] 	Mean training loss: 0.8408.
[ Fri Apr 20 20:08:30 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:08:30 2018 ] Training epoch: 14
[ Fri Apr 20 20:08:47 2018 ] 	Batch(0/627) done. Loss: 0.6936  lr:0.010000
[ Fri Apr 20 20:08:57 2018 ] 	Batch(100/627) done. Loss: 0.6483  lr:0.010000
[ Fri Apr 20 20:09:08 2018 ] 	Batch(200/627) done. Loss: 0.7468  lr:0.010000
[ Fri Apr 20 20:09:18 2018 ] 	Batch(300/627) done. Loss: 1.0256  lr:0.010000
[ Fri Apr 20 20:09:29 2018 ] 	Batch(400/627) done. Loss: 0.7874  lr:0.010000
[ Fri Apr 20 20:09:39 2018 ] 	Batch(500/627) done. Loss: 0.9252  lr:0.010000
[ Fri Apr 20 20:09:49 2018 ] 	Batch(600/627) done. Loss: 0.8524  lr:0.010000
[ Fri Apr 20 20:09:52 2018 ] 	Mean training loss: 0.8217.
[ Fri Apr 20 20:09:52 2018 ] 	Time consumption: [Data]24%, [Network]76%
[ Fri Apr 20 20:09:52 2018 ] Training epoch: 15
[ Fri Apr 20 20:10:09 2018 ] 	Batch(0/627) done. Loss: 0.8037  lr:0.010000
[ Fri Apr 20 20:10:19 2018 ] 	Batch(100/627) done. Loss: 0.8844  lr:0.010000
[ Fri Apr 20 20:10:29 2018 ] 	Batch(200/627) done. Loss: 0.8727  lr:0.010000
[ Fri Apr 20 20:10:39 2018 ] 	Batch(300/627) done. Loss: 0.5564  lr:0.010000
[ Fri Apr 20 20:10:49 2018 ] 	Batch(400/627) done. Loss: 0.6790  lr:0.010000
[ Fri Apr 20 20:10:59 2018 ] 	Batch(500/627) done. Loss: 0.7942  lr:0.010000
[ Fri Apr 20 20:11:09 2018 ] 	Batch(600/627) done. Loss: 0.6839  lr:0.010000
[ Fri Apr 20 20:11:11 2018 ] 	Mean training loss: 0.8122.
[ Fri Apr 20 20:11:11 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 20:11:11 2018 ] Eval epoch: 15
[ Fri Apr 20 20:11:33 2018 ] 	Mean test loss of 258 batches: 1.016717988622281.
[ Fri Apr 20 20:11:33 2018 ] 	Top1: 69.93%
[ Fri Apr 20 20:11:33 2018 ] 	Top5: 92.97%
[ Fri Apr 20 20:11:33 2018 ] Training epoch: 16
[ Fri Apr 20 20:11:50 2018 ] 	Batch(0/627) done. Loss: 1.1919  lr:0.010000
[ Fri Apr 20 20:12:01 2018 ] 	Batch(100/627) done. Loss: 1.0359  lr:0.010000
[ Fri Apr 20 20:12:12 2018 ] 	Batch(200/627) done. Loss: 0.7632  lr:0.010000
[ Fri Apr 20 20:12:22 2018 ] 	Batch(300/627) done. Loss: 0.4918  lr:0.010000
[ Fri Apr 20 20:12:32 2018 ] 	Batch(400/627) done. Loss: 0.9884  lr:0.010000
[ Fri Apr 20 20:12:42 2018 ] 	Batch(500/627) done. Loss: 0.8321  lr:0.010000
[ Fri Apr 20 20:12:52 2018 ] 	Batch(600/627) done. Loss: 0.8540  lr:0.010000
[ Fri Apr 20 20:12:54 2018 ] 	Mean training loss: 0.7990.
[ Fri Apr 20 20:12:54 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:12:54 2018 ] Training epoch: 17
[ Fri Apr 20 20:13:12 2018 ] 	Batch(0/627) done. Loss: 0.7258  lr:0.010000
[ Fri Apr 20 20:13:22 2018 ] 	Batch(100/627) done. Loss: 0.7872  lr:0.010000
[ Fri Apr 20 20:13:33 2018 ] 	Batch(200/627) done. Loss: 0.9608  lr:0.010000
[ Fri Apr 20 20:13:43 2018 ] 	Batch(300/627) done. Loss: 0.9329  lr:0.010000
[ Fri Apr 20 20:13:53 2018 ] 	Batch(400/627) done. Loss: 0.9374  lr:0.010000
[ Fri Apr 20 20:14:04 2018 ] 	Batch(500/627) done. Loss: 0.6191  lr:0.010000
[ Fri Apr 20 20:14:14 2018 ] 	Batch(600/627) done. Loss: 0.9971  lr:0.010000
[ Fri Apr 20 20:14:16 2018 ] 	Mean training loss: 0.7922.
[ Fri Apr 20 20:14:16 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:14:16 2018 ] Training epoch: 18
[ Fri Apr 20 20:14:34 2018 ] 	Batch(0/627) done. Loss: 0.9397  lr:0.010000
[ Fri Apr 20 20:14:44 2018 ] 	Batch(100/627) done. Loss: 0.6454  lr:0.010000
[ Fri Apr 20 20:14:55 2018 ] 	Batch(200/627) done. Loss: 0.7830  lr:0.010000
[ Fri Apr 20 20:15:05 2018 ] 	Batch(300/627) done. Loss: 0.8661  lr:0.010000
[ Fri Apr 20 20:15:15 2018 ] 	Batch(400/627) done. Loss: 0.7919  lr:0.010000
[ Fri Apr 20 20:15:26 2018 ] 	Batch(500/627) done. Loss: 0.7812  lr:0.010000
[ Fri Apr 20 20:15:36 2018 ] 	Batch(600/627) done. Loss: 0.6427  lr:0.010000
[ Fri Apr 20 20:15:39 2018 ] 	Mean training loss: 0.7810.
[ Fri Apr 20 20:15:39 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:15:39 2018 ] Training epoch: 19
[ Fri Apr 20 20:15:56 2018 ] 	Batch(0/627) done. Loss: 1.0764  lr:0.010000
[ Fri Apr 20 20:16:07 2018 ] 	Batch(100/627) done. Loss: 0.8302  lr:0.010000
[ Fri Apr 20 20:16:17 2018 ] 	Batch(200/627) done. Loss: 0.6061  lr:0.010000
[ Fri Apr 20 20:16:27 2018 ] 	Batch(300/627) done. Loss: 0.8278  lr:0.010000
[ Fri Apr 20 20:16:37 2018 ] 	Batch(400/627) done. Loss: 0.6385  lr:0.010000
[ Fri Apr 20 20:16:47 2018 ] 	Batch(500/627) done. Loss: 0.7844  lr:0.010000
[ Fri Apr 20 20:16:58 2018 ] 	Batch(600/627) done. Loss: 0.7989  lr:0.010000
[ Fri Apr 20 20:17:00 2018 ] 	Mean training loss: 0.7722.
[ Fri Apr 20 20:17:00 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:17:00 2018 ] Training epoch: 20
[ Fri Apr 20 20:17:18 2018 ] 	Batch(0/627) done. Loss: 0.6053  lr:0.010000
[ Fri Apr 20 20:17:28 2018 ] 	Batch(100/627) done. Loss: 0.8685  lr:0.010000
[ Fri Apr 20 20:17:38 2018 ] 	Batch(200/627) done. Loss: 0.6091  lr:0.010000
[ Fri Apr 20 20:17:48 2018 ] 	Batch(300/627) done. Loss: 0.9740  lr:0.010000
[ Fri Apr 20 20:17:58 2018 ] 	Batch(400/627) done. Loss: 0.8590  lr:0.010000
[ Fri Apr 20 20:18:08 2018 ] 	Batch(500/627) done. Loss: 0.8233  lr:0.010000
[ Fri Apr 20 20:18:18 2018 ] 	Batch(600/627) done. Loss: 0.6093  lr:0.010000
[ Fri Apr 20 20:18:21 2018 ] 	Mean training loss: 0.7599.
[ Fri Apr 20 20:18:21 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:18:21 2018 ] Eval epoch: 20
[ Fri Apr 20 20:18:41 2018 ] 	Mean test loss of 258 batches: 0.9867870618206586.
[ Fri Apr 20 20:18:41 2018 ] 	Top1: 70.93%
[ Fri Apr 20 20:18:42 2018 ] 	Top5: 93.34%
[ Fri Apr 20 20:18:42 2018 ] Training epoch: 21
[ Fri Apr 20 20:18:59 2018 ] 	Batch(0/627) done. Loss: 0.5904  lr:0.010000
[ Fri Apr 20 20:19:09 2018 ] 	Batch(100/627) done. Loss: 0.5934  lr:0.010000
[ Fri Apr 20 20:19:19 2018 ] 	Batch(200/627) done. Loss: 0.8386  lr:0.010000
[ Fri Apr 20 20:19:29 2018 ] 	Batch(300/627) done. Loss: 0.9984  lr:0.010000
[ Fri Apr 20 20:19:37 2018 ] 	Batch(400/627) done. Loss: 1.0097  lr:0.010000
[ Fri Apr 20 20:19:47 2018 ] 	Batch(500/627) done. Loss: 0.5782  lr:0.010000
[ Fri Apr 20 20:19:55 2018 ] 	Batch(600/627) done. Loss: 0.8768  lr:0.010000
[ Fri Apr 20 20:19:57 2018 ] 	Mean training loss: 0.7496.
[ Fri Apr 20 20:19:57 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 20:19:57 2018 ] Training epoch: 22
[ Fri Apr 20 20:20:14 2018 ] 	Batch(0/627) done. Loss: 0.9097  lr:0.010000
[ Fri Apr 20 20:20:24 2018 ] 	Batch(100/627) done. Loss: 0.5454  lr:0.010000
[ Fri Apr 20 20:20:33 2018 ] 	Batch(200/627) done. Loss: 0.8167  lr:0.010000
[ Fri Apr 20 20:20:41 2018 ] 	Batch(300/627) done. Loss: 0.7028  lr:0.010000
[ Fri Apr 20 20:20:51 2018 ] 	Batch(400/627) done. Loss: 0.8094  lr:0.010000
[ Fri Apr 20 20:21:01 2018 ] 	Batch(500/627) done. Loss: 0.5810  lr:0.010000
[ Fri Apr 20 20:21:11 2018 ] 	Batch(600/627) done. Loss: 0.6616  lr:0.010000
[ Fri Apr 20 20:21:13 2018 ] 	Mean training loss: 0.7403.
[ Fri Apr 20 20:21:13 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 20:21:13 2018 ] Training epoch: 23
[ Fri Apr 20 20:21:31 2018 ] 	Batch(0/627) done. Loss: 0.8738  lr:0.010000
[ Fri Apr 20 20:21:41 2018 ] 	Batch(100/627) done. Loss: 0.6952  lr:0.010000
[ Fri Apr 20 20:21:51 2018 ] 	Batch(200/627) done. Loss: 0.7885  lr:0.010000
[ Fri Apr 20 20:22:01 2018 ] 	Batch(300/627) done. Loss: 0.8911  lr:0.010000
[ Fri Apr 20 20:22:12 2018 ] 	Batch(400/627) done. Loss: 1.0202  lr:0.010000
[ Fri Apr 20 20:22:22 2018 ] 	Batch(500/627) done. Loss: 0.7059  lr:0.010000
[ Fri Apr 20 20:22:32 2018 ] 	Batch(600/627) done. Loss: 0.8883  lr:0.010000
[ Fri Apr 20 20:22:35 2018 ] 	Mean training loss: 0.7320.
[ Fri Apr 20 20:22:35 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:22:35 2018 ] Training epoch: 24
[ Fri Apr 20 20:22:52 2018 ] 	Batch(0/627) done. Loss: 0.6934  lr:0.010000
[ Fri Apr 20 20:23:01 2018 ] 	Batch(100/627) done. Loss: 0.8801  lr:0.010000
[ Fri Apr 20 20:23:11 2018 ] 	Batch(200/627) done. Loss: 0.5491  lr:0.010000
[ Fri Apr 20 20:23:21 2018 ] 	Batch(300/627) done. Loss: 0.8796  lr:0.010000
[ Fri Apr 20 20:23:30 2018 ] 	Batch(400/627) done. Loss: 0.7889  lr:0.010000
[ Fri Apr 20 20:23:40 2018 ] 	Batch(500/627) done. Loss: 0.5975  lr:0.010000
[ Fri Apr 20 20:23:50 2018 ] 	Batch(600/627) done. Loss: 0.5926  lr:0.010000
[ Fri Apr 20 20:23:52 2018 ] 	Mean training loss: 0.7190.
[ Fri Apr 20 20:23:52 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 20:23:52 2018 ] Training epoch: 25
[ Fri Apr 20 20:24:10 2018 ] 	Batch(0/627) done. Loss: 0.7073  lr:0.010000
[ Fri Apr 20 20:24:19 2018 ] 	Batch(100/627) done. Loss: 0.6196  lr:0.010000
[ Fri Apr 20 20:24:26 2018 ] 	Batch(200/627) done. Loss: 0.6236  lr:0.010000
[ Fri Apr 20 20:24:35 2018 ] 	Batch(300/627) done. Loss: 0.5721  lr:0.010000
[ Fri Apr 20 20:24:44 2018 ] 	Batch(400/627) done. Loss: 0.7877  lr:0.010000
[ Fri Apr 20 20:24:53 2018 ] 	Batch(500/627) done. Loss: 0.8186  lr:0.010000
[ Fri Apr 20 20:25:02 2018 ] 	Batch(600/627) done. Loss: 0.9009  lr:0.010000
[ Fri Apr 20 20:25:04 2018 ] 	Mean training loss: 0.7154.
[ Fri Apr 20 20:25:04 2018 ] 	Time consumption: [Data]29%, [Network]71%
[ Fri Apr 20 20:25:04 2018 ] Eval epoch: 25
[ Fri Apr 20 20:25:26 2018 ] 	Mean test loss of 258 batches: 0.9712570011615753.
[ Fri Apr 20 20:25:26 2018 ] 	Top1: 71.57%
[ Fri Apr 20 20:25:27 2018 ] 	Top5: 93.41%
[ Fri Apr 20 20:25:27 2018 ] Training epoch: 26
[ Fri Apr 20 20:25:43 2018 ] 	Batch(0/627) done. Loss: 0.6164  lr:0.010000
[ Fri Apr 20 20:25:54 2018 ] 	Batch(100/627) done. Loss: 0.6035  lr:0.010000
[ Fri Apr 20 20:26:04 2018 ] 	Batch(200/627) done. Loss: 0.4398  lr:0.010000
[ Fri Apr 20 20:26:14 2018 ] 	Batch(300/627) done. Loss: 0.6378  lr:0.010000
[ Fri Apr 20 20:26:24 2018 ] 	Batch(400/627) done. Loss: 0.4341  lr:0.010000
[ Fri Apr 20 20:26:33 2018 ] 	Batch(500/627) done. Loss: 0.9203  lr:0.010000
[ Fri Apr 20 20:26:41 2018 ] 	Batch(600/627) done. Loss: 0.6807  lr:0.010000
[ Fri Apr 20 20:26:43 2018 ] 	Mean training loss: 0.7085.
[ Fri Apr 20 20:26:43 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 20:26:43 2018 ] Training epoch: 27
[ Fri Apr 20 20:27:01 2018 ] 	Batch(0/627) done. Loss: 0.6668  lr:0.010000
[ Fri Apr 20 20:27:11 2018 ] 	Batch(100/627) done. Loss: 0.7682  lr:0.010000
[ Fri Apr 20 20:27:22 2018 ] 	Batch(200/627) done. Loss: 0.7778  lr:0.010000
[ Fri Apr 20 20:27:32 2018 ] 	Batch(300/627) done. Loss: 0.7814  lr:0.010000
[ Fri Apr 20 20:27:42 2018 ] 	Batch(400/627) done. Loss: 0.9742  lr:0.010000
[ Fri Apr 20 20:27:52 2018 ] 	Batch(500/627) done. Loss: 0.6806  lr:0.010000
[ Fri Apr 20 20:28:03 2018 ] 	Batch(600/627) done. Loss: 0.6854  lr:0.010000
[ Fri Apr 20 20:28:05 2018 ] 	Mean training loss: 0.7009.
[ Fri Apr 20 20:28:05 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:28:05 2018 ] Training epoch: 28
[ Fri Apr 20 20:28:23 2018 ] 	Batch(0/627) done. Loss: 0.6810  lr:0.010000
[ Fri Apr 20 20:28:33 2018 ] 	Batch(100/627) done. Loss: 0.7839  lr:0.010000
[ Fri Apr 20 20:28:43 2018 ] 	Batch(200/627) done. Loss: 0.8393  lr:0.010000
[ Fri Apr 20 20:28:54 2018 ] 	Batch(300/627) done. Loss: 0.5709  lr:0.010000
[ Fri Apr 20 20:29:04 2018 ] 	Batch(400/627) done. Loss: 0.7434  lr:0.010000
[ Fri Apr 20 20:29:14 2018 ] 	Batch(500/627) done. Loss: 0.8700  lr:0.010000
[ Fri Apr 20 20:29:24 2018 ] 	Batch(600/627) done. Loss: 0.6808  lr:0.010000
[ Fri Apr 20 20:29:26 2018 ] 	Mean training loss: 0.6920.
[ Fri Apr 20 20:29:26 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:29:26 2018 ] Training epoch: 29
[ Fri Apr 20 20:29:44 2018 ] 	Batch(0/627) done. Loss: 0.5655  lr:0.010000
[ Fri Apr 20 20:29:53 2018 ] 	Batch(100/627) done. Loss: 0.4935  lr:0.010000
[ Fri Apr 20 20:30:03 2018 ] 	Batch(200/627) done. Loss: 0.6586  lr:0.010000
[ Fri Apr 20 20:30:13 2018 ] 	Batch(300/627) done. Loss: 0.5916  lr:0.010000
[ Fri Apr 20 20:30:23 2018 ] 	Batch(400/627) done. Loss: 0.7021  lr:0.010000
[ Fri Apr 20 20:30:32 2018 ] 	Batch(500/627) done. Loss: 0.6275  lr:0.010000
[ Fri Apr 20 20:30:42 2018 ] 	Batch(600/627) done. Loss: 0.6986  lr:0.010000
[ Fri Apr 20 20:30:45 2018 ] 	Mean training loss: 0.6816.
[ Fri Apr 20 20:30:45 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:30:45 2018 ] Training epoch: 30
[ Fri Apr 20 20:31:02 2018 ] 	Batch(0/627) done. Loss: 0.6628  lr:0.010000
[ Fri Apr 20 20:31:12 2018 ] 	Batch(100/627) done. Loss: 0.6539  lr:0.010000
[ Fri Apr 20 20:31:22 2018 ] 	Batch(200/627) done. Loss: 0.6626  lr:0.010000
[ Fri Apr 20 20:31:31 2018 ] 	Batch(300/627) done. Loss: 0.6942  lr:0.010000
[ Fri Apr 20 20:31:40 2018 ] 	Batch(400/627) done. Loss: 0.5713  lr:0.010000
[ Fri Apr 20 20:31:49 2018 ] 	Batch(500/627) done. Loss: 0.8024  lr:0.010000
[ Fri Apr 20 20:31:57 2018 ] 	Batch(600/627) done. Loss: 0.7841  lr:0.010000
[ Fri Apr 20 20:32:00 2018 ] 	Mean training loss: 0.6725.
[ Fri Apr 20 20:32:00 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 20:32:00 2018 ] Eval epoch: 30
[ Fri Apr 20 20:32:22 2018 ] 	Mean test loss of 258 batches: 0.9534024011719134.
[ Fri Apr 20 20:32:22 2018 ] 	Top1: 72.33%
[ Fri Apr 20 20:32:22 2018 ] 	Top5: 93.77%
[ Fri Apr 20 20:32:22 2018 ] Training epoch: 31
[ Fri Apr 20 20:32:40 2018 ] 	Batch(0/627) done. Loss: 0.8381  lr:0.010000
[ Fri Apr 20 20:32:50 2018 ] 	Batch(100/627) done. Loss: 0.7344  lr:0.010000
[ Fri Apr 20 20:33:01 2018 ] 	Batch(200/627) done. Loss: 0.8793  lr:0.010000
[ Fri Apr 20 20:33:11 2018 ] 	Batch(300/627) done. Loss: 0.5290  lr:0.010000
[ Fri Apr 20 20:33:21 2018 ] 	Batch(400/627) done. Loss: 0.4832  lr:0.010000
[ Fri Apr 20 20:33:32 2018 ] 	Batch(500/627) done. Loss: 0.6347  lr:0.010000
[ Fri Apr 20 20:33:41 2018 ] 	Batch(600/627) done. Loss: 0.8836  lr:0.010000
[ Fri Apr 20 20:33:43 2018 ] 	Mean training loss: 0.6682.
[ Fri Apr 20 20:33:43 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:33:43 2018 ] Training epoch: 32
[ Fri Apr 20 20:34:00 2018 ] 	Batch(0/627) done. Loss: 0.6366  lr:0.010000
[ Fri Apr 20 20:34:09 2018 ] 	Batch(100/627) done. Loss: 0.5693  lr:0.010000
[ Fri Apr 20 20:34:18 2018 ] 	Batch(200/627) done. Loss: 0.7391  lr:0.010000
[ Fri Apr 20 20:34:26 2018 ] 	Batch(300/627) done. Loss: 0.6732  lr:0.010000
[ Fri Apr 20 20:34:35 2018 ] 	Batch(400/627) done. Loss: 0.5495  lr:0.010000
[ Fri Apr 20 20:34:43 2018 ] 	Batch(500/627) done. Loss: 0.4657  lr:0.010000
[ Fri Apr 20 20:34:52 2018 ] 	Batch(600/627) done. Loss: 0.7274  lr:0.010000
[ Fri Apr 20 20:34:54 2018 ] 	Mean training loss: 0.6576.
[ Fri Apr 20 20:34:54 2018 ] 	Time consumption: [Data]28%, [Network]72%
[ Fri Apr 20 20:34:54 2018 ] Training epoch: 33
[ Fri Apr 20 20:35:11 2018 ] 	Batch(0/627) done. Loss: 0.7357  lr:0.010000
[ Fri Apr 20 20:35:22 2018 ] 	Batch(100/627) done. Loss: 0.7112  lr:0.010000
[ Fri Apr 20 20:35:32 2018 ] 	Batch(200/627) done. Loss: 0.6683  lr:0.010000
[ Fri Apr 20 20:35:42 2018 ] 	Batch(300/627) done. Loss: 0.5557  lr:0.010000
[ Fri Apr 20 20:35:52 2018 ] 	Batch(400/627) done. Loss: 0.8201  lr:0.010000
[ Fri Apr 20 20:36:02 2018 ] 	Batch(500/627) done. Loss: 0.6124  lr:0.010000
[ Fri Apr 20 20:36:13 2018 ] 	Batch(600/627) done. Loss: 0.7108  lr:0.010000
[ Fri Apr 20 20:36:15 2018 ] 	Mean training loss: 0.6530.
[ Fri Apr 20 20:36:15 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:36:15 2018 ] Training epoch: 34
[ Fri Apr 20 20:36:33 2018 ] 	Batch(0/627) done. Loss: 0.6972  lr:0.010000
[ Fri Apr 20 20:36:43 2018 ] 	Batch(100/627) done. Loss: 0.5830  lr:0.010000
[ Fri Apr 20 20:36:53 2018 ] 	Batch(200/627) done. Loss: 0.7163  lr:0.010000
[ Fri Apr 20 20:37:04 2018 ] 	Batch(300/627) done. Loss: 0.5995  lr:0.010000
[ Fri Apr 20 20:37:14 2018 ] 	Batch(400/627) done. Loss: 0.6940  lr:0.010000
[ Fri Apr 20 20:37:24 2018 ] 	Batch(500/627) done. Loss: 0.7693  lr:0.010000
[ Fri Apr 20 20:37:34 2018 ] 	Batch(600/627) done. Loss: 0.8088  lr:0.010000
[ Fri Apr 20 20:37:37 2018 ] 	Mean training loss: 0.6451.
[ Fri Apr 20 20:37:37 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:37:37 2018 ] Training epoch: 35
[ Fri Apr 20 20:37:54 2018 ] 	Batch(0/627) done. Loss: 0.6203  lr:0.010000
[ Fri Apr 20 20:38:03 2018 ] 	Batch(100/627) done. Loss: 0.5104  lr:0.010000
[ Fri Apr 20 20:38:11 2018 ] 	Batch(200/627) done. Loss: 0.8826  lr:0.010000
[ Fri Apr 20 20:38:20 2018 ] 	Batch(300/627) done. Loss: 0.5364  lr:0.010000
[ Fri Apr 20 20:38:28 2018 ] 	Batch(400/627) done. Loss: 0.5887  lr:0.010000
[ Fri Apr 20 20:38:37 2018 ] 	Batch(500/627) done. Loss: 0.4484  lr:0.010000
[ Fri Apr 20 20:38:46 2018 ] 	Batch(600/627) done. Loss: 0.4755  lr:0.010000
[ Fri Apr 20 20:38:49 2018 ] 	Mean training loss: 0.6420.
[ Fri Apr 20 20:38:49 2018 ] 	Time consumption: [Data]28%, [Network]72%
[ Fri Apr 20 20:38:49 2018 ] Eval epoch: 35
[ Fri Apr 20 20:39:11 2018 ] 	Mean test loss of 258 batches: 0.9650710897390232.
[ Fri Apr 20 20:39:11 2018 ] 	Top1: 72.45%
[ Fri Apr 20 20:39:11 2018 ] 	Top5: 93.83%
[ Fri Apr 20 20:39:11 2018 ] Training epoch: 36
[ Fri Apr 20 20:39:29 2018 ] 	Batch(0/627) done. Loss: 0.5815  lr:0.010000
[ Fri Apr 20 20:39:39 2018 ] 	Batch(100/627) done. Loss: 0.7635  lr:0.010000
[ Fri Apr 20 20:39:49 2018 ] 	Batch(200/627) done. Loss: 0.8455  lr:0.010000
[ Fri Apr 20 20:39:59 2018 ] 	Batch(300/627) done. Loss: 0.6664  lr:0.010000
[ Fri Apr 20 20:40:09 2018 ] 	Batch(400/627) done. Loss: 0.4556  lr:0.010000
[ Fri Apr 20 20:40:19 2018 ] 	Batch(500/627) done. Loss: 0.5593  lr:0.010000
[ Fri Apr 20 20:40:29 2018 ] 	Batch(600/627) done. Loss: 0.3907  lr:0.010000
[ Fri Apr 20 20:40:31 2018 ] 	Mean training loss: 0.6310.
[ Fri Apr 20 20:40:31 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:40:31 2018 ] Training epoch: 37
[ Fri Apr 20 20:40:49 2018 ] 	Batch(0/627) done. Loss: 0.5683  lr:0.010000
[ Fri Apr 20 20:40:59 2018 ] 	Batch(100/627) done. Loss: 0.7045  lr:0.010000
[ Fri Apr 20 20:41:10 2018 ] 	Batch(200/627) done. Loss: 0.4866  lr:0.010000
[ Fri Apr 20 20:41:20 2018 ] 	Batch(300/627) done. Loss: 0.6244  lr:0.010000
[ Fri Apr 20 20:41:30 2018 ] 	Batch(400/627) done. Loss: 0.6674  lr:0.010000
[ Fri Apr 20 20:41:40 2018 ] 	Batch(500/627) done. Loss: 0.7549  lr:0.010000
[ Fri Apr 20 20:41:50 2018 ] 	Batch(600/627) done. Loss: 0.5505  lr:0.010000
[ Fri Apr 20 20:41:53 2018 ] 	Mean training loss: 0.6232.
[ Fri Apr 20 20:41:53 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:41:53 2018 ] Training epoch: 38
[ Fri Apr 20 20:42:11 2018 ] 	Batch(0/627) done. Loss: 0.6412  lr:0.010000
[ Fri Apr 20 20:42:21 2018 ] 	Batch(100/627) done. Loss: 0.4333  lr:0.010000
[ Fri Apr 20 20:42:31 2018 ] 	Batch(200/627) done. Loss: 0.5572  lr:0.010000
[ Fri Apr 20 20:42:41 2018 ] 	Batch(300/627) done. Loss: 0.4718  lr:0.010000
[ Fri Apr 20 20:42:52 2018 ] 	Batch(400/627) done. Loss: 0.6491  lr:0.010000
[ Fri Apr 20 20:43:02 2018 ] 	Batch(500/627) done. Loss: 0.8742  lr:0.010000
[ Fri Apr 20 20:43:12 2018 ] 	Batch(600/627) done. Loss: 0.4817  lr:0.010000
[ Fri Apr 20 20:43:15 2018 ] 	Mean training loss: 0.6168.
[ Fri Apr 20 20:43:15 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:43:15 2018 ] Training epoch: 39
[ Fri Apr 20 20:43:32 2018 ] 	Batch(0/627) done. Loss: 0.5839  lr:0.010000
[ Fri Apr 20 20:43:42 2018 ] 	Batch(100/627) done. Loss: 0.5759  lr:0.010000
[ Fri Apr 20 20:43:53 2018 ] 	Batch(200/627) done. Loss: 0.4590  lr:0.010000
[ Fri Apr 20 20:44:03 2018 ] 	Batch(300/627) done. Loss: 0.6244  lr:0.010000
[ Fri Apr 20 20:44:13 2018 ] 	Batch(400/627) done. Loss: 0.4981  lr:0.010000
[ Fri Apr 20 20:44:24 2018 ] 	Batch(500/627) done. Loss: 0.5196  lr:0.010000
[ Fri Apr 20 20:44:33 2018 ] 	Batch(600/627) done. Loss: 0.6632  lr:0.010000
[ Fri Apr 20 20:44:36 2018 ] 	Mean training loss: 0.6079.
[ Fri Apr 20 20:44:36 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:44:36 2018 ] Training epoch: 40
[ Fri Apr 20 20:44:54 2018 ] 	Batch(0/627) done. Loss: 0.5711  lr:0.010000
[ Fri Apr 20 20:45:03 2018 ] 	Batch(100/627) done. Loss: 0.6529  lr:0.010000
[ Fri Apr 20 20:45:11 2018 ] 	Batch(200/627) done. Loss: 0.7870  lr:0.010000
[ Fri Apr 20 20:45:20 2018 ] 	Batch(300/627) done. Loss: 0.6037  lr:0.010000
[ Fri Apr 20 20:45:29 2018 ] 	Batch(400/627) done. Loss: 0.7109  lr:0.010000
[ Fri Apr 20 20:45:38 2018 ] 	Batch(500/627) done. Loss: 0.8976  lr:0.010000
[ Fri Apr 20 20:45:47 2018 ] 	Batch(600/627) done. Loss: 0.6239  lr:0.010000
[ Fri Apr 20 20:45:49 2018 ] 	Mean training loss: 0.6103.
[ Fri Apr 20 20:45:49 2018 ] 	Time consumption: [Data]28%, [Network]72%
[ Fri Apr 20 20:45:49 2018 ] Eval epoch: 40
[ Fri Apr 20 20:46:11 2018 ] 	Mean test loss of 258 batches: 0.9573696097431257.
[ Fri Apr 20 20:46:11 2018 ] 	Top1: 72.55%
[ Fri Apr 20 20:46:11 2018 ] 	Top5: 93.80%
[ Fri Apr 20 20:46:11 2018 ] Training epoch: 41
[ Fri Apr 20 20:46:28 2018 ] 	Batch(0/627) done. Loss: 0.6622  lr:0.010000
[ Fri Apr 20 20:46:39 2018 ] 	Batch(100/627) done. Loss: 0.6008  lr:0.010000
[ Fri Apr 20 20:46:49 2018 ] 	Batch(200/627) done. Loss: 0.6764  lr:0.010000
[ Fri Apr 20 20:46:59 2018 ] 	Batch(300/627) done. Loss: 0.4506  lr:0.010000
[ Fri Apr 20 20:47:09 2018 ] 	Batch(400/627) done. Loss: 0.5391  lr:0.010000
[ Fri Apr 20 20:47:20 2018 ] 	Batch(500/627) done. Loss: 0.6213  lr:0.010000
[ Fri Apr 20 20:47:30 2018 ] 	Batch(600/627) done. Loss: 0.4232  lr:0.010000
[ Fri Apr 20 20:47:32 2018 ] 	Mean training loss: 0.6019.
[ Fri Apr 20 20:47:32 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:47:32 2018 ] Training epoch: 42
[ Fri Apr 20 20:47:50 2018 ] 	Batch(0/627) done. Loss: 0.6551  lr:0.010000
[ Fri Apr 20 20:48:00 2018 ] 	Batch(100/627) done. Loss: 0.3522  lr:0.010000
[ Fri Apr 20 20:48:09 2018 ] 	Batch(200/627) done. Loss: 0.4418  lr:0.010000
[ Fri Apr 20 20:48:17 2018 ] 	Batch(300/627) done. Loss: 0.6502  lr:0.010000
[ Fri Apr 20 20:48:26 2018 ] 	Batch(400/627) done. Loss: 0.5833  lr:0.010000
[ Fri Apr 20 20:48:36 2018 ] 	Batch(500/627) done. Loss: 0.5717  lr:0.010000
[ Fri Apr 20 20:48:45 2018 ] 	Batch(600/627) done. Loss: 0.5211  lr:0.010000
[ Fri Apr 20 20:48:48 2018 ] 	Mean training loss: 0.5927.
[ Fri Apr 20 20:48:48 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 20:48:48 2018 ] Training epoch: 43
[ Fri Apr 20 20:49:05 2018 ] 	Batch(0/627) done. Loss: 0.4640  lr:0.010000
[ Fri Apr 20 20:49:16 2018 ] 	Batch(100/627) done. Loss: 0.5068  lr:0.010000
[ Fri Apr 20 20:49:26 2018 ] 	Batch(200/627) done. Loss: 0.6740  lr:0.010000
[ Fri Apr 20 20:49:37 2018 ] 	Batch(300/627) done. Loss: 0.8134  lr:0.010000
[ Fri Apr 20 20:49:47 2018 ] 	Batch(400/627) done. Loss: 0.4046  lr:0.010000
[ Fri Apr 20 20:49:57 2018 ] 	Batch(500/627) done. Loss: 0.4930  lr:0.010000
[ Fri Apr 20 20:50:07 2018 ] 	Batch(600/627) done. Loss: 0.4331  lr:0.010000
[ Fri Apr 20 20:50:10 2018 ] 	Mean training loss: 0.5873.
[ Fri Apr 20 20:50:10 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:50:10 2018 ] Training epoch: 44
[ Fri Apr 20 20:50:28 2018 ] 	Batch(0/627) done. Loss: 0.8644  lr:0.010000
[ Fri Apr 20 20:50:38 2018 ] 	Batch(100/627) done. Loss: 0.6037  lr:0.010000
[ Fri Apr 20 20:50:48 2018 ] 	Batch(200/627) done. Loss: 0.6533  lr:0.010000
[ Fri Apr 20 20:50:58 2018 ] 	Batch(300/627) done. Loss: 0.4601  lr:0.010000
[ Fri Apr 20 20:51:08 2018 ] 	Batch(400/627) done. Loss: 0.5764  lr:0.010000
[ Fri Apr 20 20:51:18 2018 ] 	Batch(500/627) done. Loss: 0.6397  lr:0.010000
[ Fri Apr 20 20:51:29 2018 ] 	Batch(600/627) done. Loss: 0.7000  lr:0.010000
[ Fri Apr 20 20:51:32 2018 ] 	Mean training loss: 0.5835.
[ Fri Apr 20 20:51:32 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:51:32 2018 ] Training epoch: 45
[ Fri Apr 20 20:51:49 2018 ] 	Batch(0/627) done. Loss: 0.3539  lr:0.010000
[ Fri Apr 20 20:51:58 2018 ] 	Batch(100/627) done. Loss: 0.5903  lr:0.010000
[ Fri Apr 20 20:52:08 2018 ] 	Batch(200/627) done. Loss: 0.5120  lr:0.010000
[ Fri Apr 20 20:52:17 2018 ] 	Batch(300/627) done. Loss: 0.5267  lr:0.010000
[ Fri Apr 20 20:52:27 2018 ] 	Batch(400/627) done. Loss: 0.6144  lr:0.010000
[ Fri Apr 20 20:52:35 2018 ] 	Batch(500/627) done. Loss: 0.4066  lr:0.010000
[ Fri Apr 20 20:52:44 2018 ] 	Batch(600/627) done. Loss: 0.6258  lr:0.010000
[ Fri Apr 20 20:52:47 2018 ] 	Mean training loss: 0.5719.
[ Fri Apr 20 20:52:47 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 20:52:47 2018 ] Eval epoch: 45
[ Fri Apr 20 20:53:08 2018 ] 	Mean test loss of 258 batches: 0.9725129909293596.
[ Fri Apr 20 20:53:08 2018 ] 	Top1: 71.92%
[ Fri Apr 20 20:53:08 2018 ] 	Top5: 93.76%
[ Fri Apr 20 20:53:08 2018 ] Training epoch: 46
[ Fri Apr 20 20:53:25 2018 ] 	Batch(0/627) done. Loss: 0.5331  lr:0.010000
[ Fri Apr 20 20:53:36 2018 ] 	Batch(100/627) done. Loss: 0.4655  lr:0.010000
[ Fri Apr 20 20:53:46 2018 ] 	Batch(200/627) done. Loss: 0.4852  lr:0.010000
[ Fri Apr 20 20:53:56 2018 ] 	Batch(300/627) done. Loss: 0.5720  lr:0.010000
[ Fri Apr 20 20:54:06 2018 ] 	Batch(400/627) done. Loss: 0.4935  lr:0.010000
[ Fri Apr 20 20:54:16 2018 ] 	Batch(500/627) done. Loss: 0.6403  lr:0.010000
[ Fri Apr 20 20:54:26 2018 ] 	Batch(600/627) done. Loss: 0.6183  lr:0.010000
[ Fri Apr 20 20:54:29 2018 ] 	Mean training loss: 0.5692.
[ Fri Apr 20 20:54:29 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:54:29 2018 ] Training epoch: 47
[ Fri Apr 20 20:54:46 2018 ] 	Batch(0/627) done. Loss: 0.4150  lr:0.010000
[ Fri Apr 20 20:54:56 2018 ] 	Batch(100/627) done. Loss: 0.4203  lr:0.010000
[ Fri Apr 20 20:55:04 2018 ] 	Batch(200/627) done. Loss: 0.5803  lr:0.010000
[ Fri Apr 20 20:55:14 2018 ] 	Batch(300/627) done. Loss: 0.5128  lr:0.010000
[ Fri Apr 20 20:55:24 2018 ] 	Batch(400/627) done. Loss: 0.6658  lr:0.010000
[ Fri Apr 20 20:55:34 2018 ] 	Batch(500/627) done. Loss: 0.5665  lr:0.010000
[ Fri Apr 20 20:55:44 2018 ] 	Batch(600/627) done. Loss: 0.5411  lr:0.010000
[ Fri Apr 20 20:55:47 2018 ] 	Mean training loss: 0.5614.
[ Fri Apr 20 20:55:47 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 20:55:47 2018 ] Training epoch: 48
[ Fri Apr 20 20:56:04 2018 ] 	Batch(0/627) done. Loss: 0.5414  lr:0.010000
[ Fri Apr 20 20:56:15 2018 ] 	Batch(100/627) done. Loss: 0.5336  lr:0.010000
[ Fri Apr 20 20:56:25 2018 ] 	Batch(200/627) done. Loss: 0.5127  lr:0.010000
[ Fri Apr 20 20:56:34 2018 ] 	Batch(300/627) done. Loss: 0.4485  lr:0.010000
[ Fri Apr 20 20:56:44 2018 ] 	Batch(400/627) done. Loss: 0.4921  lr:0.010000
[ Fri Apr 20 20:56:53 2018 ] 	Batch(500/627) done. Loss: 0.3822  lr:0.010000
[ Fri Apr 20 20:57:03 2018 ] 	Batch(600/627) done. Loss: 0.6305  lr:0.010000
[ Fri Apr 20 20:57:06 2018 ] 	Mean training loss: 0.5611.
[ Fri Apr 20 20:57:06 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:57:06 2018 ] Training epoch: 49
[ Fri Apr 20 20:57:23 2018 ] 	Batch(0/627) done. Loss: 0.8714  lr:0.010000
[ Fri Apr 20 20:57:34 2018 ] 	Batch(100/627) done. Loss: 0.6397  lr:0.010000
[ Fri Apr 20 20:57:44 2018 ] 	Batch(200/627) done. Loss: 0.4379  lr:0.010000
[ Fri Apr 20 20:57:54 2018 ] 	Batch(300/627) done. Loss: 0.5164  lr:0.010000
[ Fri Apr 20 20:58:03 2018 ] 	Batch(400/627) done. Loss: 0.5715  lr:0.010000
[ Fri Apr 20 20:58:13 2018 ] 	Batch(500/627) done. Loss: 0.6156  lr:0.010000
[ Fri Apr 20 20:58:23 2018 ] 	Batch(600/627) done. Loss: 0.4730  lr:0.010000
[ Fri Apr 20 20:58:26 2018 ] 	Mean training loss: 0.5535.
[ Fri Apr 20 20:58:26 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:58:26 2018 ] Training epoch: 50
[ Fri Apr 20 20:58:43 2018 ] 	Batch(0/627) done. Loss: 0.5776  lr:0.010000
[ Fri Apr 20 20:58:54 2018 ] 	Batch(100/627) done. Loss: 0.5292  lr:0.010000
[ Fri Apr 20 20:59:04 2018 ] 	Batch(200/627) done. Loss: 0.6474  lr:0.010000
[ Fri Apr 20 20:59:14 2018 ] 	Batch(300/627) done. Loss: 0.6553  lr:0.010000
[ Fri Apr 20 20:59:24 2018 ] 	Batch(400/627) done. Loss: 0.8228  lr:0.010000
[ Fri Apr 20 20:59:33 2018 ] 	Batch(500/627) done. Loss: 0.4881  lr:0.010000
[ Fri Apr 20 20:59:43 2018 ] 	Batch(600/627) done. Loss: 0.7125  lr:0.010000
[ Fri Apr 20 20:59:45 2018 ] 	Mean training loss: 0.5500.
[ Fri Apr 20 20:59:45 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 20:59:45 2018 ] Eval epoch: 50
[ Fri Apr 20 21:00:07 2018 ] 	Mean test loss of 258 batches: 0.9499066200598266.
[ Fri Apr 20 21:00:07 2018 ] 	Top1: 72.92%
[ Fri Apr 20 21:00:08 2018 ] 	Top5: 93.99%
[ Fri Apr 20 21:00:08 2018 ] Training epoch: 51
[ Fri Apr 20 21:00:25 2018 ] 	Batch(0/627) done. Loss: 0.5012  lr:0.010000
[ Fri Apr 20 21:00:34 2018 ] 	Batch(100/627) done. Loss: 0.7696  lr:0.010000
[ Fri Apr 20 21:00:43 2018 ] 	Batch(200/627) done. Loss: 0.5122  lr:0.010000
[ Fri Apr 20 21:00:52 2018 ] 	Batch(300/627) done. Loss: 0.5857  lr:0.010000
[ Fri Apr 20 21:00:58 2018 ] 	Batch(400/627) done. Loss: 0.6878  lr:0.010000
[ Fri Apr 20 21:01:08 2018 ] 	Batch(500/627) done. Loss: 0.3785  lr:0.010000
[ Fri Apr 20 21:01:18 2018 ] 	Batch(600/627) done. Loss: 0.5057  lr:0.010000
[ Fri Apr 20 21:01:21 2018 ] 	Mean training loss: 0.5415.
[ Fri Apr 20 21:01:21 2018 ] 	Time consumption: [Data]28%, [Network]72%
[ Fri Apr 20 21:01:21 2018 ] Training epoch: 52
[ Fri Apr 20 21:01:38 2018 ] 	Batch(0/627) done. Loss: 0.6365  lr:0.010000
[ Fri Apr 20 21:01:49 2018 ] 	Batch(100/627) done. Loss: 0.6534  lr:0.010000
[ Fri Apr 20 21:01:59 2018 ] 	Batch(200/627) done. Loss: 0.2927  lr:0.010000
[ Fri Apr 20 21:02:09 2018 ] 	Batch(300/627) done. Loss: 0.4901  lr:0.010000
[ Fri Apr 20 21:02:19 2018 ] 	Batch(400/627) done. Loss: 0.6071  lr:0.010000
[ Fri Apr 20 21:02:29 2018 ] 	Batch(500/627) done. Loss: 0.5425  lr:0.010000
[ Fri Apr 20 21:02:39 2018 ] 	Batch(600/627) done. Loss: 0.4602  lr:0.010000
[ Fri Apr 20 21:02:41 2018 ] 	Mean training loss: 0.5362.
[ Fri Apr 20 21:02:41 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:02:41 2018 ] Training epoch: 53
[ Fri Apr 20 21:02:59 2018 ] 	Batch(0/627) done. Loss: 0.3427  lr:0.010000
[ Fri Apr 20 21:03:09 2018 ] 	Batch(100/627) done. Loss: 0.6120  lr:0.010000
[ Fri Apr 20 21:03:18 2018 ] 	Batch(200/627) done. Loss: 0.6985  lr:0.010000
[ Fri Apr 20 21:03:28 2018 ] 	Batch(300/627) done. Loss: 0.5648  lr:0.010000
[ Fri Apr 20 21:03:38 2018 ] 	Batch(400/627) done. Loss: 0.3577  lr:0.010000
[ Fri Apr 20 21:03:48 2018 ] 	Batch(500/627) done. Loss: 0.5069  lr:0.010000
[ Fri Apr 20 21:03:58 2018 ] 	Batch(600/627) done. Loss: 0.4715  lr:0.010000
[ Fri Apr 20 21:04:01 2018 ] 	Mean training loss: 0.5279.
[ Fri Apr 20 21:04:01 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 21:04:01 2018 ] Training epoch: 54
[ Fri Apr 20 21:04:18 2018 ] 	Batch(0/627) done. Loss: 0.3938  lr:0.010000
[ Fri Apr 20 21:04:28 2018 ] 	Batch(100/627) done. Loss: 0.4347  lr:0.010000
[ Fri Apr 20 21:04:36 2018 ] 	Batch(200/627) done. Loss: 0.4173  lr:0.010000
[ Fri Apr 20 21:04:46 2018 ] 	Batch(300/627) done. Loss: 0.6909  lr:0.010000
[ Fri Apr 20 21:04:56 2018 ] 	Batch(400/627) done. Loss: 0.4290  lr:0.010000
[ Fri Apr 20 21:05:06 2018 ] 	Batch(500/627) done. Loss: 0.6194  lr:0.010000
[ Fri Apr 20 21:05:16 2018 ] 	Batch(600/627) done. Loss: 0.4289  lr:0.010000
[ Fri Apr 20 21:05:19 2018 ] 	Mean training loss: 0.5247.
[ Fri Apr 20 21:05:19 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 21:05:19 2018 ] Training epoch: 55
[ Fri Apr 20 21:05:36 2018 ] 	Batch(0/627) done. Loss: 0.5462  lr:0.010000
[ Fri Apr 20 21:05:47 2018 ] 	Batch(100/627) done. Loss: 0.6728  lr:0.010000
[ Fri Apr 20 21:05:57 2018 ] 	Batch(200/627) done. Loss: 0.3622  lr:0.010000
[ Fri Apr 20 21:06:05 2018 ] 	Batch(300/627) done. Loss: 0.4330  lr:0.010000
[ Fri Apr 20 21:06:14 2018 ] 	Batch(400/627) done. Loss: 0.5324  lr:0.010000
[ Fri Apr 20 21:06:23 2018 ] 	Batch(500/627) done. Loss: 0.5062  lr:0.010000
[ Fri Apr 20 21:06:32 2018 ] 	Batch(600/627) done. Loss: 0.5128  lr:0.010000
[ Fri Apr 20 21:06:34 2018 ] 	Mean training loss: 0.5189.
[ Fri Apr 20 21:06:34 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 21:06:34 2018 ] Eval epoch: 55
[ Fri Apr 20 21:06:56 2018 ] 	Mean test loss of 258 batches: 0.967330123333968.
[ Fri Apr 20 21:06:56 2018 ] 	Top1: 72.97%
[ Fri Apr 20 21:06:56 2018 ] 	Top5: 93.89%
[ Fri Apr 20 21:06:56 2018 ] Training epoch: 56
[ Fri Apr 20 21:07:14 2018 ] 	Batch(0/627) done. Loss: 0.4347  lr:0.010000
[ Fri Apr 20 21:07:24 2018 ] 	Batch(100/627) done. Loss: 0.2993  lr:0.010000
[ Fri Apr 20 21:07:34 2018 ] 	Batch(200/627) done. Loss: 0.5118  lr:0.010000
[ Fri Apr 20 21:07:44 2018 ] 	Batch(300/627) done. Loss: 0.4551  lr:0.010000
[ Fri Apr 20 21:07:54 2018 ] 	Batch(400/627) done. Loss: 0.5353  lr:0.010000
[ Fri Apr 20 21:08:04 2018 ] 	Batch(500/627) done. Loss: 0.3935  lr:0.010000
[ Fri Apr 20 21:08:14 2018 ] 	Batch(600/627) done. Loss: 0.6721  lr:0.010000
[ Fri Apr 20 21:08:16 2018 ] 	Mean training loss: 0.5207.
[ Fri Apr 20 21:08:16 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 21:08:16 2018 ] Training epoch: 57
[ Fri Apr 20 21:08:33 2018 ] 	Batch(0/627) done. Loss: 0.5061  lr:0.010000
[ Fri Apr 20 21:08:43 2018 ] 	Batch(100/627) done. Loss: 0.5838  lr:0.010000
[ Fri Apr 20 21:08:54 2018 ] 	Batch(200/627) done. Loss: 0.4086  lr:0.010000
[ Fri Apr 20 21:09:04 2018 ] 	Batch(300/627) done. Loss: 0.7014  lr:0.010000
[ Fri Apr 20 21:09:14 2018 ] 	Batch(400/627) done. Loss: 0.6482  lr:0.010000
[ Fri Apr 20 21:09:25 2018 ] 	Batch(500/627) done. Loss: 0.4557  lr:0.010000
[ Fri Apr 20 21:09:35 2018 ] 	Batch(600/627) done. Loss: 0.4631  lr:0.010000
[ Fri Apr 20 21:09:37 2018 ] 	Mean training loss: 0.5135.
[ Fri Apr 20 21:09:37 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:09:37 2018 ] Training epoch: 58
[ Fri Apr 20 21:09:55 2018 ] 	Batch(0/627) done. Loss: 0.6246  lr:0.010000
[ Fri Apr 20 21:10:04 2018 ] 	Batch(100/627) done. Loss: 0.4848  lr:0.010000
[ Fri Apr 20 21:10:14 2018 ] 	Batch(200/627) done. Loss: 0.3570  lr:0.010000
[ Fri Apr 20 21:10:25 2018 ] 	Batch(300/627) done. Loss: 0.4134  lr:0.010000
[ Fri Apr 20 21:10:35 2018 ] 	Batch(400/627) done. Loss: 0.6046  lr:0.010000
[ Fri Apr 20 21:10:45 2018 ] 	Batch(500/627) done. Loss: 0.5942  lr:0.010000
[ Fri Apr 20 21:10:55 2018 ] 	Batch(600/627) done. Loss: 0.5923  lr:0.010000
[ Fri Apr 20 21:10:58 2018 ] 	Mean training loss: 0.5002.
[ Fri Apr 20 21:10:58 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:10:58 2018 ] Training epoch: 59
[ Fri Apr 20 21:11:15 2018 ] 	Batch(0/627) done. Loss: 0.3988  lr:0.010000
[ Fri Apr 20 21:11:25 2018 ] 	Batch(100/627) done. Loss: 0.7320  lr:0.010000
[ Fri Apr 20 21:11:34 2018 ] 	Batch(200/627) done. Loss: 0.4125  lr:0.010000
[ Fri Apr 20 21:11:44 2018 ] 	Batch(300/627) done. Loss: 0.3856  lr:0.010000
[ Fri Apr 20 21:11:53 2018 ] 	Batch(400/627) done. Loss: 0.7891  lr:0.010000
[ Fri Apr 20 21:12:03 2018 ] 	Batch(500/627) done. Loss: 0.5725  lr:0.010000
[ Fri Apr 20 21:12:13 2018 ] 	Batch(600/627) done. Loss: 0.4133  lr:0.010000
[ Fri Apr 20 21:12:15 2018 ] 	Mean training loss: 0.5033.
[ Fri Apr 20 21:12:15 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 21:12:15 2018 ] Training epoch: 60
[ Fri Apr 20 21:12:32 2018 ] 	Batch(0/627) done. Loss: 0.6599  lr:0.010000
[ Fri Apr 20 21:12:42 2018 ] 	Batch(100/627) done. Loss: 0.4545  lr:0.010000
[ Fri Apr 20 21:12:52 2018 ] 	Batch(200/627) done. Loss: 0.7136  lr:0.010000
[ Fri Apr 20 21:13:03 2018 ] 	Batch(300/627) done. Loss: 0.4637  lr:0.010000
[ Fri Apr 20 21:13:13 2018 ] 	Batch(400/627) done. Loss: 0.6731  lr:0.010000
[ Fri Apr 20 21:13:23 2018 ] 	Batch(500/627) done. Loss: 0.4609  lr:0.010000
[ Fri Apr 20 21:13:33 2018 ] 	Batch(600/627) done. Loss: 0.6092  lr:0.010000
[ Fri Apr 20 21:13:36 2018 ] 	Mean training loss: 0.4986.
[ Fri Apr 20 21:13:36 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:13:36 2018 ] Eval epoch: 60
[ Fri Apr 20 21:13:59 2018 ] 	Mean test loss of 258 batches: 0.9555155396923538.
[ Fri Apr 20 21:13:59 2018 ] 	Top1: 73.26%
[ Fri Apr 20 21:13:59 2018 ] 	Top5: 94.20%
[ Fri Apr 20 21:13:59 2018 ] Training epoch: 61
[ Fri Apr 20 21:14:16 2018 ] 	Batch(0/627) done. Loss: 0.3674  lr:0.010000
[ Fri Apr 20 21:14:26 2018 ] 	Batch(100/627) done. Loss: 0.4179  lr:0.010000
[ Fri Apr 20 21:14:34 2018 ] 	Batch(200/627) done. Loss: 0.4681  lr:0.010000
[ Fri Apr 20 21:14:43 2018 ] 	Batch(300/627) done. Loss: 0.5863  lr:0.010000
[ Fri Apr 20 21:14:53 2018 ] 	Batch(400/627) done. Loss: 0.3114  lr:0.010000
[ Fri Apr 20 21:15:02 2018 ] 	Batch(500/627) done. Loss: 0.5228  lr:0.010000
[ Fri Apr 20 21:15:12 2018 ] 	Batch(600/627) done. Loss: 0.8985  lr:0.010000
[ Fri Apr 20 21:15:14 2018 ] 	Mean training loss: 0.4944.
[ Fri Apr 20 21:15:14 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 21:15:14 2018 ] Training epoch: 62
[ Fri Apr 20 21:15:31 2018 ] 	Batch(0/627) done. Loss: 0.3991  lr:0.010000
[ Fri Apr 20 21:15:42 2018 ] 	Batch(100/627) done. Loss: 0.6043  lr:0.010000
[ Fri Apr 20 21:15:52 2018 ] 	Batch(200/627) done. Loss: 0.5275  lr:0.010000
[ Fri Apr 20 21:16:02 2018 ] 	Batch(300/627) done. Loss: 0.5205  lr:0.010000
[ Fri Apr 20 21:16:12 2018 ] 	Batch(400/627) done. Loss: 0.4065  lr:0.010000
[ Fri Apr 20 21:16:22 2018 ] 	Batch(500/627) done. Loss: 0.5083  lr:0.010000
[ Fri Apr 20 21:16:32 2018 ] 	Batch(600/627) done. Loss: 0.4742  lr:0.010000
[ Fri Apr 20 21:16:34 2018 ] 	Mean training loss: 0.4849.
[ Fri Apr 20 21:16:34 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:16:34 2018 ] Training epoch: 63
[ Fri Apr 20 21:16:52 2018 ] 	Batch(0/627) done. Loss: 0.4543  lr:0.010000
[ Fri Apr 20 21:17:01 2018 ] 	Batch(100/627) done. Loss: 0.3816  lr:0.010000
[ Fri Apr 20 21:17:10 2018 ] 	Batch(200/627) done. Loss: 0.4945  lr:0.010000
[ Fri Apr 20 21:17:18 2018 ] 	Batch(300/627) done. Loss: 0.4222  lr:0.010000
[ Fri Apr 20 21:17:28 2018 ] 	Batch(400/627) done. Loss: 0.5618  lr:0.010000
[ Fri Apr 20 21:17:37 2018 ] 	Batch(500/627) done. Loss: 0.5262  lr:0.010000
[ Fri Apr 20 21:17:47 2018 ] 	Batch(600/627) done. Loss: 0.5334  lr:0.010000
[ Fri Apr 20 21:17:49 2018 ] 	Mean training loss: 0.4897.
[ Fri Apr 20 21:17:49 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 21:17:49 2018 ] Training epoch: 64
[ Fri Apr 20 21:18:07 2018 ] 	Batch(0/627) done. Loss: 0.3731  lr:0.010000
[ Fri Apr 20 21:18:17 2018 ] 	Batch(100/627) done. Loss: 0.6040  lr:0.010000
[ Fri Apr 20 21:18:27 2018 ] 	Batch(200/627) done. Loss: 0.3404  lr:0.010000
[ Fri Apr 20 21:18:38 2018 ] 	Batch(300/627) done. Loss: 0.3697  lr:0.010000
[ Fri Apr 20 21:18:48 2018 ] 	Batch(400/627) done. Loss: 0.3876  lr:0.010000
[ Fri Apr 20 21:18:58 2018 ] 	Batch(500/627) done. Loss: 0.3463  lr:0.010000
[ Fri Apr 20 21:19:08 2018 ] 	Batch(600/627) done. Loss: 0.4774  lr:0.010000
[ Fri Apr 20 21:19:11 2018 ] 	Mean training loss: 0.4815.
[ Fri Apr 20 21:19:11 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:19:11 2018 ] Training epoch: 65
[ Fri Apr 20 21:19:28 2018 ] 	Batch(0/627) done. Loss: 0.3152  lr:0.010000
[ Fri Apr 20 21:19:37 2018 ] 	Batch(100/627) done. Loss: 0.4211  lr:0.010000
[ Fri Apr 20 21:19:47 2018 ] 	Batch(200/627) done. Loss: 0.5044  lr:0.010000
[ Fri Apr 20 21:19:57 2018 ] 	Batch(300/627) done. Loss: 0.5989  lr:0.010000
[ Fri Apr 20 21:20:07 2018 ] 	Batch(400/627) done. Loss: 0.5286  lr:0.010000
[ Fri Apr 20 21:20:16 2018 ] 	Batch(500/627) done. Loss: 0.3713  lr:0.010000
[ Fri Apr 20 21:20:26 2018 ] 	Batch(600/627) done. Loss: 0.6400  lr:0.010000
[ Fri Apr 20 21:20:29 2018 ] 	Mean training loss: 0.4796.
[ Fri Apr 20 21:20:29 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 21:20:29 2018 ] Eval epoch: 65
[ Fri Apr 20 21:20:51 2018 ] 	Mean test loss of 258 batches: 0.9497893279375031.
[ Fri Apr 20 21:20:51 2018 ] 	Top1: 73.60%
[ Fri Apr 20 21:20:51 2018 ] 	Top5: 94.20%
[ Fri Apr 20 21:20:51 2018 ] Training epoch: 66
[ Fri Apr 20 21:21:08 2018 ] 	Batch(0/627) done. Loss: 0.4250  lr:0.010000
[ Fri Apr 20 21:21:19 2018 ] 	Batch(100/627) done. Loss: 0.4487  lr:0.010000
[ Fri Apr 20 21:21:29 2018 ] 	Batch(200/627) done. Loss: 0.3908  lr:0.010000
[ Fri Apr 20 21:21:39 2018 ] 	Batch(300/627) done. Loss: 0.5524  lr:0.010000
[ Fri Apr 20 21:21:48 2018 ] 	Batch(400/627) done. Loss: 0.4147  lr:0.010000
[ Fri Apr 20 21:21:57 2018 ] 	Batch(500/627) done. Loss: 0.7324  lr:0.010000
[ Fri Apr 20 21:22:06 2018 ] 	Batch(600/627) done. Loss: 0.3211  lr:0.010000
[ Fri Apr 20 21:22:08 2018 ] 	Mean training loss: 0.4719.
[ Fri Apr 20 21:22:08 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 21:22:08 2018 ] Training epoch: 67
[ Fri Apr 20 21:22:25 2018 ] 	Batch(0/627) done. Loss: 0.4156  lr:0.010000
[ Fri Apr 20 21:22:35 2018 ] 	Batch(100/627) done. Loss: 0.3989  lr:0.010000
[ Fri Apr 20 21:22:43 2018 ] 	Batch(200/627) done. Loss: 0.3513  lr:0.010000
[ Fri Apr 20 21:22:52 2018 ] 	Batch(300/627) done. Loss: 0.4589  lr:0.010000
[ Fri Apr 20 21:23:01 2018 ] 	Batch(400/627) done. Loss: 0.7341  lr:0.010000
[ Fri Apr 20 21:23:10 2018 ] 	Batch(500/627) done. Loss: 0.4307  lr:0.010000
[ Fri Apr 20 21:23:18 2018 ] 	Batch(600/627) done. Loss: 0.6020  lr:0.010000
[ Fri Apr 20 21:23:20 2018 ] 	Mean training loss: 0.4716.
[ Fri Apr 20 21:23:20 2018 ] 	Time consumption: [Data]29%, [Network]71%
[ Fri Apr 20 21:23:20 2018 ] Training epoch: 68
[ Fri Apr 20 21:23:37 2018 ] 	Batch(0/627) done. Loss: 0.6494  lr:0.010000
[ Fri Apr 20 21:23:47 2018 ] 	Batch(100/627) done. Loss: 0.4637  lr:0.010000
[ Fri Apr 20 21:23:57 2018 ] 	Batch(200/627) done. Loss: 0.5366  lr:0.010000
[ Fri Apr 20 21:24:06 2018 ] 	Batch(300/627) done. Loss: 0.4893  lr:0.010000
[ Fri Apr 20 21:24:16 2018 ] 	Batch(400/627) done. Loss: 0.3196  lr:0.010000
[ Fri Apr 20 21:24:26 2018 ] 	Batch(500/627) done. Loss: 0.4909  lr:0.010000
[ Fri Apr 20 21:24:36 2018 ] 	Batch(600/627) done. Loss: 0.5643  lr:0.010000
[ Fri Apr 20 21:24:39 2018 ] 	Mean training loss: 0.4597.
[ Fri Apr 20 21:24:39 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 21:24:39 2018 ] Training epoch: 69
[ Fri Apr 20 21:24:56 2018 ] 	Batch(0/627) done. Loss: 0.3342  lr:0.010000
[ Fri Apr 20 21:25:06 2018 ] 	Batch(100/627) done. Loss: 0.3790  lr:0.010000
[ Fri Apr 20 21:25:16 2018 ] 	Batch(200/627) done. Loss: 0.5264  lr:0.010000
[ Fri Apr 20 21:25:27 2018 ] 	Batch(300/627) done. Loss: 0.5430  lr:0.010000
[ Fri Apr 20 21:25:37 2018 ] 	Batch(400/627) done. Loss: 0.5370  lr:0.010000
[ Fri Apr 20 21:25:47 2018 ] 	Batch(500/627) done. Loss: 0.6427  lr:0.010000
[ Fri Apr 20 21:25:57 2018 ] 	Batch(600/627) done. Loss: 0.5275  lr:0.010000
[ Fri Apr 20 21:26:00 2018 ] 	Mean training loss: 0.4596.
[ Fri Apr 20 21:26:00 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:26:00 2018 ] Training epoch: 70
[ Fri Apr 20 21:26:17 2018 ] 	Batch(0/627) done. Loss: 0.5199  lr:0.010000
[ Fri Apr 20 21:26:27 2018 ] 	Batch(100/627) done. Loss: 0.5979  lr:0.010000
[ Fri Apr 20 21:26:36 2018 ] 	Batch(200/627) done. Loss: 0.3927  lr:0.010000
[ Fri Apr 20 21:26:46 2018 ] 	Batch(300/627) done. Loss: 0.4468  lr:0.010000
[ Fri Apr 20 21:26:56 2018 ] 	Batch(400/627) done. Loss: 0.5865  lr:0.010000
[ Fri Apr 20 21:27:06 2018 ] 	Batch(500/627) done. Loss: 0.4490  lr:0.010000
[ Fri Apr 20 21:27:16 2018 ] 	Batch(600/627) done. Loss: 0.4949  lr:0.010000
[ Fri Apr 20 21:27:18 2018 ] 	Mean training loss: 0.4599.
[ Fri Apr 20 21:27:18 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:27:18 2018 ] Eval epoch: 70
[ Fri Apr 20 21:27:41 2018 ] 	Mean test loss of 258 batches: 0.9626962096654168.
[ Fri Apr 20 21:27:41 2018 ] 	Top1: 73.29%
[ Fri Apr 20 21:27:41 2018 ] 	Top5: 94.21%
[ Fri Apr 20 21:27:41 2018 ] Training epoch: 71
[ Fri Apr 20 21:27:58 2018 ] 	Batch(0/627) done. Loss: 0.3772  lr:0.001000
[ Fri Apr 20 21:28:09 2018 ] 	Batch(100/627) done. Loss: 0.2712  lr:0.001000
[ Fri Apr 20 21:28:19 2018 ] 	Batch(200/627) done. Loss: 0.1505  lr:0.001000
[ Fri Apr 20 21:28:29 2018 ] 	Batch(300/627) done. Loss: 0.2638  lr:0.001000
[ Fri Apr 20 21:28:38 2018 ] 	Batch(400/627) done. Loss: 0.6904  lr:0.001000
[ Fri Apr 20 21:28:47 2018 ] 	Batch(500/627) done. Loss: 0.2875  lr:0.001000
[ Fri Apr 20 21:28:56 2018 ] 	Batch(600/627) done. Loss: 0.2565  lr:0.001000
[ Fri Apr 20 21:28:58 2018 ] 	Mean training loss: 0.3729.
[ Fri Apr 20 21:28:58 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 21:28:58 2018 ] Training epoch: 72
[ Fri Apr 20 21:29:15 2018 ] 	Batch(0/627) done. Loss: 0.2101  lr:0.001000
[ Fri Apr 20 21:29:25 2018 ] 	Batch(100/627) done. Loss: 0.3327  lr:0.001000
[ Fri Apr 20 21:29:33 2018 ] 	Batch(200/627) done. Loss: 0.3358  lr:0.001000
[ Fri Apr 20 21:29:42 2018 ] 	Batch(300/627) done. Loss: 0.3240  lr:0.001000
[ Fri Apr 20 21:29:52 2018 ] 	Batch(400/627) done. Loss: 0.2538  lr:0.001000
[ Fri Apr 20 21:30:01 2018 ] 	Batch(500/627) done. Loss: 0.2936  lr:0.001000
[ Fri Apr 20 21:30:11 2018 ] 	Batch(600/627) done. Loss: 0.2863  lr:0.001000
[ Fri Apr 20 21:30:14 2018 ] 	Mean training loss: 0.3542.
[ Fri Apr 20 21:30:14 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 21:30:14 2018 ] Training epoch: 73
[ Fri Apr 20 21:30:31 2018 ] 	Batch(0/627) done. Loss: 0.2161  lr:0.001000
[ Fri Apr 20 21:30:41 2018 ] 	Batch(100/627) done. Loss: 0.3470  lr:0.001000
[ Fri Apr 20 21:30:51 2018 ] 	Batch(200/627) done. Loss: 0.3474  lr:0.001000
[ Fri Apr 20 21:31:01 2018 ] 	Batch(300/627) done. Loss: 0.5618  lr:0.001000
[ Fri Apr 20 21:31:10 2018 ] 	Batch(400/627) done. Loss: 0.3868  lr:0.001000
[ Fri Apr 20 21:31:20 2018 ] 	Batch(500/627) done. Loss: 0.2706  lr:0.001000
[ Fri Apr 20 21:31:29 2018 ] 	Batch(600/627) done. Loss: 0.4295  lr:0.001000
[ Fri Apr 20 21:31:32 2018 ] 	Mean training loss: 0.3448.
[ Fri Apr 20 21:31:32 2018 ] 	Time consumption: [Data]26%, [Network]74%
[ Fri Apr 20 21:31:32 2018 ] Training epoch: 74
[ Fri Apr 20 21:31:49 2018 ] 	Batch(0/627) done. Loss: 0.3023  lr:0.001000
[ Fri Apr 20 21:31:59 2018 ] 	Batch(100/627) done. Loss: 0.4277  lr:0.001000
[ Fri Apr 20 21:32:09 2018 ] 	Batch(200/627) done. Loss: 0.1869  lr:0.001000
[ Fri Apr 20 21:32:19 2018 ] 	Batch(300/627) done. Loss: 0.3002  lr:0.001000
[ Fri Apr 20 21:32:28 2018 ] 	Batch(400/627) done. Loss: 0.2680  lr:0.001000
[ Fri Apr 20 21:32:38 2018 ] 	Batch(500/627) done. Loss: 0.6287  lr:0.001000
[ Fri Apr 20 21:32:48 2018 ] 	Batch(600/627) done. Loss: 0.5079  lr:0.001000
[ Fri Apr 20 21:32:51 2018 ] 	Mean training loss: 0.3363.
[ Fri Apr 20 21:32:51 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:32:51 2018 ] Training epoch: 75
[ Fri Apr 20 21:33:09 2018 ] 	Batch(0/627) done. Loss: 0.3766  lr:0.001000
[ Fri Apr 20 21:33:19 2018 ] 	Batch(100/627) done. Loss: 0.2404  lr:0.001000
[ Fri Apr 20 21:33:30 2018 ] 	Batch(200/627) done. Loss: 0.3422  lr:0.001000
[ Fri Apr 20 21:33:40 2018 ] 	Batch(300/627) done. Loss: 0.2542  lr:0.001000
[ Fri Apr 20 21:33:50 2018 ] 	Batch(400/627) done. Loss: 0.2707  lr:0.001000
[ Fri Apr 20 21:34:00 2018 ] 	Batch(500/627) done. Loss: 0.3319  lr:0.001000
[ Fri Apr 20 21:34:11 2018 ] 	Batch(600/627) done. Loss: 0.2461  lr:0.001000
[ Fri Apr 20 21:34:13 2018 ] 	Mean training loss: 0.3334.
[ Fri Apr 20 21:34:13 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:34:13 2018 ] Eval epoch: 75
[ Fri Apr 20 21:34:35 2018 ] 	Mean test loss of 258 batches: 0.8968959096097207.
[ Fri Apr 20 21:34:35 2018 ] 	Top1: 75.11%
[ Fri Apr 20 21:34:35 2018 ] 	Top5: 94.70%
[ Fri Apr 20 21:34:35 2018 ] Training epoch: 76
[ Fri Apr 20 21:34:53 2018 ] 	Batch(0/627) done. Loss: 0.2164  lr:0.001000
[ Fri Apr 20 21:35:04 2018 ] 	Batch(100/627) done. Loss: 0.2669  lr:0.001000
[ Fri Apr 20 21:35:14 2018 ] 	Batch(200/627) done. Loss: 0.2630  lr:0.001000
[ Fri Apr 20 21:35:24 2018 ] 	Batch(300/627) done. Loss: 0.2152  lr:0.001000
[ Fri Apr 20 21:35:35 2018 ] 	Batch(400/627) done. Loss: 0.4583  lr:0.001000
[ Fri Apr 20 21:35:45 2018 ] 	Batch(500/627) done. Loss: 0.3373  lr:0.001000
[ Fri Apr 20 21:35:54 2018 ] 	Batch(600/627) done. Loss: 0.5259  lr:0.001000
[ Fri Apr 20 21:35:56 2018 ] 	Mean training loss: 0.3366.
[ Fri Apr 20 21:35:56 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:35:56 2018 ] Training epoch: 77
[ Fri Apr 20 21:36:14 2018 ] 	Batch(0/627) done. Loss: 0.2726  lr:0.001000
[ Fri Apr 20 21:36:22 2018 ] 	Batch(100/627) done. Loss: 0.3946  lr:0.001000
[ Fri Apr 20 21:36:31 2018 ] 	Batch(200/627) done. Loss: 0.2874  lr:0.001000
[ Fri Apr 20 21:36:40 2018 ] 	Batch(300/627) done. Loss: 0.2906  lr:0.001000
[ Fri Apr 20 21:36:49 2018 ] 	Batch(400/627) done. Loss: 0.2955  lr:0.001000
[ Fri Apr 20 21:36:59 2018 ] 	Batch(500/627) done. Loss: 0.3519  lr:0.001000
[ Fri Apr 20 21:37:08 2018 ] 	Batch(600/627) done. Loss: 0.3424  lr:0.001000
[ Fri Apr 20 21:37:11 2018 ] 	Mean training loss: 0.3312.
[ Fri Apr 20 21:37:11 2018 ] 	Time consumption: [Data]27%, [Network]73%
[ Fri Apr 20 21:37:11 2018 ] Training epoch: 78
[ Fri Apr 20 21:37:28 2018 ] 	Batch(0/627) done. Loss: 0.3750  lr:0.001000
[ Fri Apr 20 21:37:39 2018 ] 	Batch(100/627) done. Loss: 0.4346  lr:0.001000
[ Fri Apr 20 21:37:49 2018 ] 	Batch(200/627) done. Loss: 0.3584  lr:0.001000
[ Fri Apr 20 21:37:59 2018 ] 	Batch(300/627) done. Loss: 0.4560  lr:0.001000
[ Fri Apr 20 21:38:09 2018 ] 	Batch(400/627) done. Loss: 0.3980  lr:0.001000
[ Fri Apr 20 21:38:20 2018 ] 	Batch(500/627) done. Loss: 0.3534  lr:0.001000
[ Fri Apr 20 21:38:30 2018 ] 	Batch(600/627) done. Loss: 0.4924  lr:0.001000
[ Fri Apr 20 21:38:31 2018 ] 	Mean training loss: 0.3265.
[ Fri Apr 20 21:38:31 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:38:31 2018 ] Training epoch: 79
[ Fri Apr 20 21:38:48 2018 ] 	Batch(0/627) done. Loss: 0.2498  lr:0.001000
[ Fri Apr 20 21:38:59 2018 ] 	Batch(100/627) done. Loss: 0.3418  lr:0.001000
[ Fri Apr 20 21:39:08 2018 ] 	Batch(200/627) done. Loss: 0.3881  lr:0.001000
[ Fri Apr 20 21:39:18 2018 ] 	Batch(300/627) done. Loss: 0.3980  lr:0.001000
[ Fri Apr 20 21:39:28 2018 ] 	Batch(400/627) done. Loss: 0.4874  lr:0.001000
[ Fri Apr 20 21:39:38 2018 ] 	Batch(500/627) done. Loss: 0.2343  lr:0.001000
[ Fri Apr 20 21:39:48 2018 ] 	Batch(600/627) done. Loss: 0.3526  lr:0.001000
[ Fri Apr 20 21:39:51 2018 ] 	Mean training loss: 0.3249.
[ Fri Apr 20 21:39:51 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:39:51 2018 ] Training epoch: 80
[ Fri Apr 20 21:40:08 2018 ] 	Batch(0/627) done. Loss: 0.2939  lr:0.001000
[ Fri Apr 20 21:40:19 2018 ] 	Batch(100/627) done. Loss: 0.4183  lr:0.001000
[ Fri Apr 20 21:40:29 2018 ] 	Batch(200/627) done. Loss: 0.2898  lr:0.001000
[ Fri Apr 20 21:40:39 2018 ] 	Batch(300/627) done. Loss: 0.3070  lr:0.001000
[ Fri Apr 20 21:40:49 2018 ] 	Batch(400/627) done. Loss: 0.2774  lr:0.001000
[ Fri Apr 20 21:40:59 2018 ] 	Batch(500/627) done. Loss: 0.1782  lr:0.001000
[ Fri Apr 20 21:41:10 2018 ] 	Batch(600/627) done. Loss: 0.1535  lr:0.001000
[ Fri Apr 20 21:41:12 2018 ] 	Mean training loss: 0.3234.
[ Fri Apr 20 21:41:12 2018 ] 	Time consumption: [Data]25%, [Network]75%
[ Fri Apr 20 21:41:12 2018 ] Eval epoch: 80
[ Fri Apr 20 21:41:35 2018 ] 	Mean test loss of 258 batches: 0.9005260369343351.
[ Fri Apr 20 21:41:35 2018 ] 	Top1: 74.96%
[ Fri Apr 20 21:41:35 2018 ] 	Top5: 94.89%
